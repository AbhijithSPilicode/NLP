{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b7bd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intent classification is process of classifying customer's intent by analyzing the language they use\n",
    "#it is the central aspect of chatbot conversation\n",
    "#2 phases:training phrase and preset response\n",
    "#methods for intent classification:\n",
    "#BoW\n",
    "#TF-IDF\n",
    "#CNN with Glove\n",
    "#BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "912181eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from apikey import apikey\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7dc58264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e897c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\banking77\\9898c11f6afa9521953d2ef205667b527bad14ef9cab445d470f16240c8c8ec4\\banking77.py:59: FutureWarning: Dataset 'banking77' is deprecated and will be deleted. Use 'PolyAI/banking77' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset=load_dataset(\"banking77\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f3ff7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I am still waiting on my card?', 'label': 11}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e8db456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7af0947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am still waiting on my card?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do if my card still hasn't arrived ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been waiting over a week. Is the card s...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I track my card while it is in the process...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I know if I will get my card, or if it ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>You provide support in what countries?</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>What countries are you supporting?</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>What countries are getting support?</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Are cards available in the EU?</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>Which countries are represented?</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10003 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                         I am still waiting on my card?     11\n",
       "1      What can I do if my card still hasn't arrived ...     11\n",
       "2      I have been waiting over a week. Is the card s...     11\n",
       "3      Can I track my card while it is in the process...     11\n",
       "4      How do I know if I will get my card, or if it ...     11\n",
       "...                                                  ...    ...\n",
       "9998              You provide support in what countries?     24\n",
       "9999                  What countries are you supporting?     24\n",
       "10000                What countries are getting support?     24\n",
       "10001                     Are cards available in the EU?     24\n",
       "10002                   Which countries are represented?     24\n",
       "\n",
       "[10003 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbc80ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9939d370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 13, 32, 17, 34, 46, 36, 12,  4, 14, 33, 41,  1, 49, 23, 56, 47,\n",
       "        8, 60, 75, 15, 66, 54, 40, 10, 61,  6, 16, 30, 74, 68, 38, 73, 62,\n",
       "       29, 22,  3, 28, 44, 26, 45, 42, 52, 27, 51, 25, 48, 55, 18, 63, 70,\n",
       "       67, 53, 21,  7, 64, 50, 35, 65, 71, 39, 58, 43, 72, 76, 37, 59,  5,\n",
       "       20, 31, 57,  0, 19,  9,  2, 69, 24], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=df['label'].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e7df5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af7b9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-large-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
    "\n",
    "    # Tokenize and encode the input\n",
    "    inputs = tokenizer.encode_plus(text, add_special_tokens=True,padding=True, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    # Get predictions\n",
    "    logits = model(**inputs).logits\n",
    "    print(logits)\n",
    "    # Get predicted label \n",
    "    predicted_label = labels[torch.argmax(logits).item()]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "760330fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am still waiting on my card?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "090e0b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am still waiting on my card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2572, 2145, 3403, 2006, 2026, 4003,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[-0.8534, -0.1235,  0.0649,  0.1894, -0.5885,  0.3073,  1.0742, -0.1464,\n",
      "         -0.4807, -0.9743,  0.8795,  0.1553, -0.2422, -0.5565,  0.2599, -0.3448,\n",
      "          1.0411,  0.4346, -0.2175, -0.9908, -1.1609, -0.0093,  0.6316,  0.5810,\n",
      "          0.4748, -0.3263,  0.3702, -0.3331, -0.2253,  0.3205, -0.2322, -0.1730,\n",
      "         -0.3428, -0.6191,  0.4870,  0.2795,  0.5951,  0.0585,  0.1415,  0.0868,\n",
      "         -0.3747, -0.7496,  0.2090, -0.6517,  0.5493,  0.1231,  1.0092, -0.0756,\n",
      "          0.3626, -0.0907,  0.0123,  0.2949, -0.1673, -0.2499, -0.4292,  0.5598,\n",
      "          0.3320, -0.1618, -0.2829, -0.0526,  0.7710,  0.6947,  0.6272, -0.9754,\n",
      "         -0.6762,  0.8161,  0.4322, -0.3198, -0.2883,  0.2255, -0.6115,  0.1498,\n",
      "         -1.1203, -0.2721,  0.2935,  0.5227,  0.2310]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Predicted output: 36\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input()\n",
    "    if text.lower() == \"exit\":\n",
    "            break\n",
    "# Classify the sentiment\n",
    "    output = classifier(text)\n",
    "    print(\"Predicted output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1974a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from apikey import apikey\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10109967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub,LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13c8d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e91791d225346ce96f7314970a29ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115451104e05440bb0f56d233145f2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d500ab02a345efbec7cff7965a9d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260381bb8ddc4c13b054e60c66644794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "006eb67a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n__root__\n  Got invalid task text-classification, currently only ('text2text-generation', 'text-generation', 'summarization') are supported (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19224\\337284377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhub_llm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHuggingFaceHub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"philschmid/BERT-Banking77\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\load\\serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__dict__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n__root__\n  Got invalid task text-classification, currently only ('text2text-generation', 'text-generation', 'summarization') are supported (type=value_error)"
     ]
    }
   ],
   "source": [
    "hub_llm=HuggingFaceHub(repo_id=\"philschmid/BERT-Banking77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e72a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "        input_variables=['output'],\n",
    "        template=\"The intent inside the message is {output} \",\n",
    "        model_kwargs={'temperature':0.1,'max_length':100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_chain=LLMChain(prompt=prompt,llm=hub_llm,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17595dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hub_chain.run(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8c90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
