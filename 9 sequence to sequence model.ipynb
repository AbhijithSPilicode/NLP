{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fd5104a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq (Sequence-to-Sequence) is a type of model in machine learning that is used for tasks such as machine translation, text summarization, and image captioning. \n",
    "#The model consists of two main components:Encoder &  Decoder\n",
    "#Seq2Seq models are trained using a dataset of input-output pairs, where the input is a sequence of tokens and the output is also a sequence of tokens.\n",
    "#The model is trained to maximize the likelihood of the correct output sequence given the input sequence.\n",
    "#Encoder:It uses deep neural network layers and converts the input words to corresponding hidden vectors.\n",
    "#Each vector represents the current word and the context of the word\n",
    "#decoder:It takes as input the hidden vector generated by the encoder, its own hidden states, and the current word to produce the next hidden vector and finally predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "78026e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "795f6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters (should be consistent with the trained model)\n",
    "vocab_size = 50 #size of the vocabulary (number of unique words)\n",
    "embedding_dim = 10 # specifies the dimensionality of the embedding vectors.\n",
    "hidden_units = 16 #no.of nodes in LSTM\n",
    "sequence_length = 10 #no.of squence\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "hidden2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b7242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b56adad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture (same as during training)\n",
    "#Encoder:It uses deep neural network layers and converts the input words to corresponding hidden vectors.\n",
    "encoder_inputs = Input(shape=(sequence_length,)) #input layer for the encode\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs) #defines an embedding layer that converts the input sequences into dense vectors.\n",
    "#encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "#encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding) #state_h and state_c, which are the hidden state and cell state at the final time step.\n",
    "for _ in range(num_encoder_layers):\n",
    "    #encoder_outputs, state_h, state_c = LSTM(hidden2, return_state=True)(encoder_outputs)\n",
    "    encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding) #state_h and state_c, which are the hidden state and cell state at the final time step.\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b610071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder:It takes as input the hidden vector generated by the encoder, its own hidden states, and the current word to produce the next hidden vector and finally predict the next word.\n",
    "decoder_inputs = Input(shape=(sequence_length,))\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "for _ in range(num_encoder_layers):\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "42dd226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8df1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell state allowing information to flow through the sequence \n",
    "#The hidden state in an LSTM represents the network's memory of past information.\n",
    "#the hidden state and cell state work together to enable LSTMs to process and remember sequential information.\n",
    "import numpy as np\n",
    "def predict_sequence(input_sequence):\n",
    "    input_sequence = np.array(input_sequence)\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    # Initialize the decoder's initial states with the encoder's final states\n",
    "    decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_states = [state_h, state_c]        \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states\n",
    "    )\n",
    "    \n",
    "# Start the prediction process\n",
    "    states_values = encoder_model.predict(input_sequence)\n",
    "    target_seq = np.zeros((1, 1)) #The shape (1, 1) indicates that it's a sequence of length 1 and the token index is set to 0.\n",
    "    target_seq[0, 0] = 1  #The code assigns a specific value (1 in this case) to the first token in the target sequence\n",
    "\n",
    "    decoded_sequence = []\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_values) # The decoder model is used to predict the next token in the sequence. \n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :]) #The index of the token with the highest predicted probability is chosen as the next token in the sequence. \n",
    "        decoded_sequence.append(sampled_token_index)\n",
    "\n",
    "        target_seq[0, 0] = sampled_token_index #The target_seq is updated to hold the last predicted token index.\n",
    "        states_values = [h, c]\n",
    "\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ae9f12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 388ms/step\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_57'), name='input_57', description=\"created by layer 'input_57'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted Sequence: [28, 28, 26, 12, 39, 12, 35, 36, 36, 32]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = [1, 2, 3, 4, 5, 6, 7,8,9,10]  # Example input sequence\n",
    "predicted_sequence = predict_sequence([input_sequence])\n",
    "print(\"Predicted Sequence:\", predicted_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61c62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827b53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ed0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36001c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
