{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7bd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intent classification is process of classifying customer's intent by analyzing the language they use\n",
    "#it is the central aspect of chatbot conversation\n",
    "#2 phases:training phrase and preset response\n",
    "#methods for intent classification:\n",
    "#BoW\n",
    "#TF-IDF\n",
    "#CNN with Glove\n",
    "#BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912181eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from apikey import apikey\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e897c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am still waiting on my card?</td>\n",
       "      <td>11</td>\n",
       "      <td>card_arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do if my card still hasn't arrived ...</td>\n",
       "      <td>11</td>\n",
       "      <td>card_arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been waiting over a week. Is the card s...</td>\n",
       "      <td>11</td>\n",
       "      <td>card_arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I track my card while it is in the process...</td>\n",
       "      <td>11</td>\n",
       "      <td>card_arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I know if I will get my card, or if it ...</td>\n",
       "      <td>11</td>\n",
       "      <td>card_arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>You provide support in what countries?</td>\n",
       "      <td>24</td>\n",
       "      <td>country_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>What countries are you supporting?</td>\n",
       "      <td>24</td>\n",
       "      <td>country_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>What countries are getting support?</td>\n",
       "      <td>24</td>\n",
       "      <td>country_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Are cards available in the EU?</td>\n",
       "      <td>24</td>\n",
       "      <td>country_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>Which countries are represented?</td>\n",
       "      <td>24</td>\n",
       "      <td>country_support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10003 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0                         I am still waiting on my card?     11   \n",
       "1      What can I do if my card still hasn't arrived ...     11   \n",
       "2      I have been waiting over a week. Is the card s...     11   \n",
       "3      Can I track my card while it is in the process...     11   \n",
       "4      How do I know if I will get my card, or if it ...     11   \n",
       "...                                                  ...    ...   \n",
       "9998              You provide support in what countries?     24   \n",
       "9999                  What countries are you supporting?     24   \n",
       "10000                What countries are getting support?     24   \n",
       "10001                     Are cards available in the EU?     24   \n",
       "10002                   Which countries are represented?     24   \n",
       "\n",
       "            label_text  \n",
       "0         card_arrival  \n",
       "1         card_arrival  \n",
       "2         card_arrival  \n",
       "3         card_arrival  \n",
       "4         card_arrival  \n",
       "...                ...  \n",
       "9998   country_support  \n",
       "9999   country_support  \n",
       "10000  country_support  \n",
       "10001  country_support  \n",
       "10002  country_support  \n",
       "\n",
       "[10003 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"C://Users//user//Downloads//banking77.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3ff7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am still waiting on my card?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8db456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc80ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9939d370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 13, 32, 17, 34, 46, 36, 12,  4, 14, 33, 41,  1, 49, 23, 56, 47,\n",
       "        8, 60, 75, 15, 66, 54, 40, 10, 61,  6, 16, 30, 74, 68, 38, 73, 62,\n",
       "       29, 22,  3, 28, 44, 26, 45, 42, 52, 27, 51, 25, 48, 55, 18, 63, 70,\n",
       "       67, 53, 21,  7, 64, 50, 35, 65, 71, 39, 58, 43, 72, 76, 37, 59,  5,\n",
       "       20, 31, 57,  0, 19,  9,  2, 69, 24], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=dataset['label'].unique()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7df5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af7b9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-large-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
    "\n",
    "    # Tokenize and encode the input\n",
    "    inputs = tokenizer.encode_plus(text, add_special_tokens=True,padding=True, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    # Get predictions\n",
    "    logits = model(**inputs).logits\n",
    "    print(logits)\n",
    "    # Get predicted label \n",
    "    predicted_label = labels[torch.argmax(logits).item()]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10109967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub,LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c8d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier1 = pipeline(\"text-classification\",model='philschmid/BERT-Banking77')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090e0b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My physical card is not working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2026, 3558, 4003, 2003, 2025, 2551,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[-3.2110e-01, -5.8184e-01, -4.5155e-01, -2.9002e-01, -1.2346e-01,\n",
      "         -4.5123e-02, -2.3683e-01, -3.3209e-01,  5.4906e-02, -4.4779e-01,\n",
      "          1.2798e-01, -1.1887e-01,  4.2947e-01, -4.1594e-01, -9.0766e-01,\n",
      "         -2.3303e-01, -7.4124e-01, -1.2961e-03, -7.5174e-01,  5.4699e-01,\n",
      "         -5.6319e-01, -2.2589e-01,  3.8070e-01, -9.2205e-01, -4.2229e-02,\n",
      "          2.7867e-01, -7.7479e-01, -7.1987e-01,  1.4306e-01,  6.5737e-01,\n",
      "          2.9739e-01,  2.1060e-01, -1.0367e+00, -3.7862e-01, -1.4094e-01,\n",
      "         -4.7272e-01, -1.3641e+00, -2.5729e-02,  4.0325e-01,  1.0369e+00,\n",
      "         -1.8291e-01,  1.3719e-01,  8.3828e-02, -2.1581e-01, -5.5485e-01,\n",
      "          1.3956e-01,  4.5319e-02, -1.3247e-01, -6.1731e-01,  5.7479e-01,\n",
      "         -1.8761e-01, -1.5800e-01, -1.4479e-02, -1.0738e+00,  5.0488e-01,\n",
      "          1.4065e-01, -1.3392e-01, -1.5178e-01,  3.9631e-01,  4.2554e-02,\n",
      "          2.8719e-01, -4.6826e-02, -6.3271e-01,  8.6288e-01, -5.9485e-01,\n",
      "          5.9213e-01, -1.2643e-01, -4.2139e-01, -2.3018e-01, -8.8327e-01,\n",
      "         -1.7497e-01, -5.2484e-01, -8.3442e-02,  4.9764e-01,  6.6512e-02,\n",
      "          2.3470e-01, -8.4346e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring args : (array([11, 13, 32, 17, 34, 46, 36, 12,  4, 14, 33, 41,  1, 49, 23, 56, 47,\n",
      "        8, 60, 75, 15, 66, 54, 40, 10, 61,  6, 16, 30, 74, 68, 38, 73, 62,\n",
      "       29, 22,  3, 28, 44, 26, 45, 42, 52, 27, 51, 25, 48, 55, 18, 63, 70,\n",
      "       67, 53, 21,  7, 64, 50, 35, 65, 71, 39, 58, 43, 72, 76, 37, 59,  5,\n",
      "       20, 31, 57,  0, 19,  9,  2, 69, 24], dtype=int64),)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 26\n",
      "pre: [{'label': 'card_not_working', 'score': 0.9694022536277771}]\n",
      "exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6164,  102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "tensor([[-0.8107, -0.0374,  0.0395,  0.0300,  0.0706,  0.0691, -0.6840,  0.6431,\n",
      "         -0.2955,  0.3342,  0.1211, -0.8487, -0.6035,  0.4615, -0.1597,  0.0761,\n",
      "         -0.1429, -0.1070,  0.4512, -0.7345,  0.4440, -0.4092, -0.1472,  0.1507,\n",
      "          0.2428, -0.9915, -0.5736, -0.2929, -0.0506,  0.5300, -0.2767, -0.6839,\n",
      "          0.2797,  0.6603,  0.8959,  0.3008,  0.4198, -0.5329, -0.5453,  0.2416,\n",
      "         -0.6002, -0.8692,  1.0410, -0.0572,  0.2593, -1.0521, -0.9508, -0.8959,\n",
      "          0.9136,  0.9227, -0.0106,  0.6754, -0.3909, -0.0628, -0.3833,  0.2750,\n",
      "         -0.2144,  0.2833,  0.0673, -0.5535,  0.0250,  0.3396, -0.4446, -1.0846,\n",
      "          0.7820,  0.7228, -0.4019, -0.3517, -0.2438, -0.1078,  0.4628, -0.0279,\n",
      "         -0.6756, -0.6901, -0.6912, -0.9729, -0.5241]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring args : (array([11, 13, 32, 17, 34, 46, 36, 12,  4, 14, 33, 41,  1, 49, 23, 56, 47,\n",
      "        8, 60, 75, 15, 66, 54, 40, 10, 61,  6, 16, 30, 74, 68, 38, 73, 62,\n",
      "       29, 22,  3, 28, 44, 26, 45, 42, 52, 27, 51, 25, 48, 55, 18, 63, 70,\n",
      "       67, 53, 21,  7, 64, 50, 35, 65, 71, 39, 58, 43, 72, 76, 37, 59,  5,\n",
      "       20, 31, 57,  0, 19,  9,  2, 69, 24], dtype=int64),)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 52\n",
      "pre: [{'label': 'terminate_account', 'score': 0.36598843336105347}]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input()\n",
    "    output = classifier(text)\n",
    "    print(\"Predicted output:\",output)\n",
    "    output2=classifier1(text,labels)\n",
    "    print(\"pre:\",output2)\n",
    "    if text.lower() == \"exit\":\n",
    "            break\n",
    "# Classify the sentiment\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8ab2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My physical card is not working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2026, 3558, 4003, 2003, 2025, 2551,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring args : (array([11, 13, 32, 17, 34, 46, 36, 12,  4, 14, 33, 41,  1, 49, 23, 56, 47,\n",
      "        8, 60, 75, 15, 66, 54, 40, 10, 61,  6, 16, 30, 74, 68, 38, 73, 62,\n",
      "       29, 22,  3, 28, 44, 26, 45, 42, 52, 27, 51, 25, 48, 55, 18, 63, 70,\n",
      "       67, 53, 21,  7, 64, 50, 35, 65, 71, 39, 58, 43, 72, 76, 37, 59,  5,\n",
      "       20, 31, 57,  0, 19,  9,  2, 69, 24], dtype=int64),)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2997,  0.8251, -0.3508, -0.0477, -0.3247,  0.5797,  0.0114,  0.3524,\n",
      "          0.1216, -0.1002,  0.6737,  0.7951,  0.0487, -0.0994,  0.4256, -0.0263,\n",
      "          0.3419,  0.7232, -0.1111, -1.2739, -0.0215, -0.1961, -0.6637, -0.3702,\n",
      "         -0.8544,  0.5752,  0.0453, -0.5544, -0.4626, -1.1888,  0.0869, -0.1233,\n",
      "          0.1498, -0.1999, -0.0202,  0.2709,  0.1859,  0.3523,  0.0703,  0.0868,\n",
      "          0.3504, -0.2090,  0.9531,  0.2005,  0.2020,  1.0799,  0.3824,  0.7938,\n",
      "          0.9139,  0.2063, -0.1609,  0.3809,  0.5537,  0.0058,  0.2563,  0.5713,\n",
      "          0.2388,  0.1146,  0.2622,  0.4180, -0.2100, -0.7018, -0.0454, -0.8322,\n",
      "         -0.3643,  0.2078,  0.0729, -0.3843,  0.1776,  0.0125, -1.0106,  0.2984,\n",
      "         -0.3109,  0.3639, -0.6782, -0.0866, -1.0115]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Predicted output: 25\n",
      "pre: [{'label': 'card_not_working', 'score': 0.9694022536277771}]\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "output = classifier(text)\n",
    "print(\"Predicted output:\",output)\n",
    "output2=classifier1(text,labels)\n",
    "print(\"pre:\",output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "006eb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_llm=HuggingFaceHub(repo_id=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82e72a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. 'output2' (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17680\\1900337263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m prompt=PromptTemplate(\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[0minput_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"The text is {text} and The intent inside the message is {output2} \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'temperature'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\load\\serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\v1\\main.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__dict__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. 'output2' (type=value_error)"
     ]
    }
   ],
   "source": [
    "prompt=PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"The text is {text} and The intent inside the message is {output2} \",\n",
    "        model_kwargs={'temperature':0.1,'max_length':100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a58b2ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_chain=LLMChain(prompt=prompt,llm=hub_llm,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17595dc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single string input was passed in, but this chain expects multiple inputs ({'output2', 'text'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17680\\3437430664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhub_chain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[0;32m    476\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mChain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \"\"\"\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         callback_manager = CallbackManager.configure(\n\u001b[0;32m    261\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001b[0m in \u001b[0;36mprep_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[0m_input_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_input_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_input_keys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    404\u001b[0m                     \u001b[1;34mf\"A single string input was passed in, but this chain expects \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                     \u001b[1;34mf\"multiple inputs ({_input_keys}). When a chain expects \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A single string input was passed in, but this chain expects multiple inputs ({'output2', 'text'}). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`"
     ]
    }
   ],
   "source": [
    "print(hub_chain.run(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8c90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
